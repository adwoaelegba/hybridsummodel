# -*- coding: utf-8 -*-
"""KoiosModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ow7MmpMn5JIZuA2QuO5thXyLzWheRADA

# Koios Model

# Installing libraries
"""

pip install torch transformers datasets accelerate peft bitsandbytes

pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html

pip install bs4

pip install sentence_transformers

pip install nltk

"""# Extraction Function
This function is meant to scrape the HTML from the site and feed it into the model
"""

def extraction_function(html):
    from bs4 import BeautifulSoup

    with open(html, "r", encoding="utf-8") as file:
        content = file.read()
        soup = BeautifulSoup(content, "html.parser")

    sections = {}
    current_heading = None
    current_text = []  # Initialize

    for elem in soup.find_all(["h1", "h2", "h3", "p"]):

        if elem.name in ["h1", "h2", "h3"]:
            if current_heading:
                sections[current_heading] = " ".join(current_text).strip()
            current_heading = elem.get_text().strip()
            current_text = []
        else:
            current_text.append(elem.get_text().strip())

    if current_heading and current_text:
        sections[current_heading] = " ".join(current_text).strip()


    return sections

"""Testing the extraction function"""

html_file= "/content/drive/MyDrive/corpus/htmltest/www8_hp.html"
policy=extraction_function(html_file)
for section, text in policy.items():
    print(f"## {section} ###\n{text}\n")

"""# Extractive Summarization
This uses TD-IF to select the sentences with the highest importance. The measurement for importance is based on the privacy terms
"""

import torch
import nltk
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

nltk.download('stopwords')

nltk.download('punkt_tab')

sent_model=SentenceTransformer("all-MiniLM-L6-v2")

#Key privacy terms to prioritize
privacy_keywords={
    "privacy","data","personal","information","collection","third-party","cookies","retention",
    "cookies","sharing","consent","security","policy","tracking", "advertising","partners",
    "compliance","regulation","opt-out", "security"
}

#Legal phrases that should be kept
legal_phrases={
    "third-party sharing", "right to be forgotten","legitimate interest","data retention",
    "opt-out option","cookie tracking"
}

#Stopwords to remove to improve TD-IDF scores but keep the ones that are needed for legal phrases (i.e to be)
stopwords=list(set(nltk.corpus.stopwords.words("english")).union({"shall","hereby","thereof","whereas"}) - set("to be".split()))

#Function to preprocess text, keeping legal phrases. Changed text to a single token
def preprocess_text(text):
    for phrase in legal_phrases:
        text=text.replace(phrase,phrase.replace(" ","_"))
    return text

def td_extract_summary(text, boost_factor=2.0):
    text = preprocess_text(text)
    sentences = sent_tokenize(text)
    total_sentences = len(sentences)

    if total_sentences < 5:
        return text
    if total_sentences <= 10:
        num_sentences = max(5, int(total_sentences * 0.80))
    elif total_sentences <= 20:
        num_sentences = max(9, int(total_sentences * 0.5))
    else:
        num_sentences = max(11, int(total_sentences * 0.4))

    # TF-IDF
    vectorizer = TfidfVectorizer(stop_words=stopwords)
    tdidf_matrix = vectorizer.fit_transform(sentences)
    tdidf_scores = np.asarray(tdidf_matrix.mean(axis=0)).flatten()
    word_to_tdidf = dict(zip(vectorizer.get_feature_names_out(), tdidf_scores))

    sentence_weights = []
    for sentence in sentences:
        words = sentence.lower().split()
        weight = sum(word_to_tdidf.get(word, 0) for word in words)
        if any(word in privacy_keywords for word in words):
            weight *= boost_factor
        sentence_weights.append(weight)

    sentence_weights = np.array(sentence_weights)
    max_weight = np.max(sentence_weights) if np.max(sentence_weights) > 0 else 1
    sentence_weights = (sentence_weights / max_weight) * boost_factor

    # Encode sentences
    sentence_embeddings = sent_model.encode(sentences, convert_to_tensor=True)
    doc_embedding = torch.mean(sentence_embeddings, dim=0)

    # Compute similarities
    similarities = torch.nn.functional.cosine_similarity(sentence_embeddings, doc_embedding.unsqueeze(0))

    # Move sentence_weights to the same device as similarities
    sentence_weights_tensor = torch.tensor(sentence_weights, dtype=torch.float32, device=similarities.device)
    similarities = similarities * sentence_weights_tensor

    # Select top-ranked sentences
    top_indices = torch.topk(similarities, num_sentences).indices.cpu().numpy()

    # Maintain original order of selected sentences
    top_sentences = [sentences[i].replace("_", " ") for i in sorted(top_indices)]  # Restore legal phrases

    return " ".join(top_sentences)

policy="""
As a website, we are committed to safeguarding your privacy and being transparent about how we collect and share your data with third parties. To improve your user experience, we may share certain information with trusted analytics providers who help us understand how you interact with our website, allowing us to optimize performance. Additionally, we may work with advertising networks to display personalized ads based on your browsing history and interests. These third-party networks may use cookies or other tracking technologies to gather data independently from our site. We may also share data with payment processors when you make a purchase to ensure transactions are processed securely and efficiently. In some cases, we partner with social media platforms to provide features such as social sharing or login integrations. When you interact with these platforms, your information may be shared according to their privacy policies. We also share data with service providers that assist us in delivering our services, including customer support or marketing communications. While we strive to carefully select third parties that comply with data protection laws, we cannot control how they use your data once shared. Your personal data may also be shared with law enforcement or regulatory authorities if required by law or to protect our legal rights. We ensure that any third party with whom we share data adheres to applicable data protection regulations such as the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). You have the ability to manage your data-sharing preferences by adjusting your privacy settings or using opt-out options available on our site. This includes controlling how third parties use your data for advertising purposes. We encourage you to review the privacy policies of any third-party services we work with to understand their data practices. By using our website, you consent to the sharing of your data with third parties as described in this policy. However, if you have any concerns or questions about our data-sharing practices, please feel free to contact us. Your privacy is important to us, and we strive to ensure that your data is handled responsibly.
"""

#14 sentence paragraph to test on
policy="""
As a website, we are committed to safeguarding your privacy and being transparent about how we collect and share your data with third parties. To improve your user experience, we may share certain information with trusted analytics providers who help us understand how you interact with our website, allowing us to optimize performance. Additionally, we may work with advertising networks to display personalized ads based on your browsing history and interests. These third-party networks may use cookies or other tracking technologies to gather data independently from our site. We may also share data with payment processors when you make a purchase to ensure transactions are processed securely and efficiently. In some cases, we partner with social media platforms to provide features such as social sharing or login integrations. When you interact with these platforms, your information may be shared according to their privacy policies. We also share data with service providers that assist us in delivering our services, including customer support or marketing communications. While we strive to carefully select third parties that comply with data protection laws, we cannot control how they use your data once shared. Your personal data may also be shared with law enforcement or regulatory authorities if required by law or to protect our legal rights. We ensure that any third party with whom we share data adheres to applicable data protection regulations such as the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). You have the ability to manage your data-sharing preferences by adjusting your privacy settings or using opt-out options available on our site. This includes controlling how third parties use your data for advertising purposes. We encourage you to review the privacy policies of any third-party services we work with to understand their data practices. By using our website, you consent to the sharing of your data with third parties as described in this policy. However, if you have any concerns or questions about our data-sharing practices, please feel free to contact us. Your privacy is important to us, and we strive to ensure that your data is handled responsibly.
"""

summary=td_extract_summary(policy)
print("Extractive summary:", summary)

"""# Abstractive Summarization"""

import torch
from transformers import BartForConditionalGeneration, BartTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
from peft import get_peft_model, LoraConfig, TaskType

model_type="facebook/bart-large-cnn"
tokenizer=BartTokenizer.from_pretrained(model_type)
model=BartForConditionalGeneration.from_pretrained(model_type)

#Apply LoRA
lora_config= LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=12,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none"
)

model = get_peft_model(model, lora_config)

from google.colab import drive
drive.mount('/content/drive')

privacy_policies = "/content/drive/MyDrive/corpus/training/privacy_final_policies.jsonl"

import json

file_path = "/content/drive/MyDrive/corpus/training/privacy_final_policies.jsonl"

# Read the JSONL file
data = []
with open(file_path, "r", encoding="utf-8") as f:
    for line in f:
        data.append(json.loads(line))  # Load each line as a dictionary

# Print the first few examples
for i, entry in enumerate(data[:3]):
    print(f"Example {i+1}:")
    print("Input:", entry["input"])
    print("Output:", entry["output"])
    print("="*50)

dataset = load_dataset("json", data_files=privacy_policies, split="train")
print(dataset)

print(dataset[0])  # Prints the first example

#Tokenization function (old but used in getting the tokenized dataset)
def tokenize(policies):
    inputs = tokenizer(policies["input"], max_length=1024, truncation=True)
    labels = tokenizer(policies["output"], max_length=256, truncation=True)
    inputs["labels"] = labels["input_ids"]
    return inputs

print(model.config.max_position_embeddings)

#New tokenization function
def tokenization(policies):
    inputs = tokenizer(
        policies["input"],
        padding="max_length",
        truncation=True,
        max_length=1024
    )

    labels = tokenizer(
        policies["output"],
        padding="max_length",
        truncation=True,
        max_length=1024
    )

    return {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "labels": labels["input_ids"]
    }

tokenized_policies=dataset.map(tokenize, batched=True)

# Training arguments
train_args = TrainingArguments(
    output_dir="./bart_lora_privacy",
    evaluation_strategy="no",
    learning_rate=2e-4,
    per_device_train_batch_size=4,
    num_train_epochs=3,
    save_strategy="epoch",
    fp16=True,
    logging_dir="./logs"
)

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    pad_to_multiple_of=8
)

model_trainer = Trainer(
    model=model,
    args=train_args,
    train_dataset=tokenized_policies,
    data_collator=data_collator
)

import os
os.environ["WANDB_DISABLED"] = "false"

model_trainer.train()

# Saving the fine-tuned model
model.save_pretrained("/content/drive/MyDrive/corpus/Koios Model")
tokenizer.save_pretrained("/content/drive/MyDrive/corpus/Koios Model")

"""# Final Integration Pipeline
This is the combination of the 3 separate functions that have been developed individually, **the extractive function for the model, extractive summarization model and finally, the abstractive model.** Now, this pipeline combines each of them and produces modular output(summary by section)
"""

#Load model
koios_model_path="/content/drive/MyDrive/corpus/Koios Model"
tokenizer=BartTokenizer.from_pretrained(koios_model_path)
koios_model=BartForConditionalGeneration.from_pretrained(koios_model_path)

#Testing using a document in the dataset
url="/content/drive/MyDrive/corpus/htmltest/www8_hp.html"
extracted_summary=extraction_function(url)
final_summary={}


for current_heading, current_text in extracted_summary.items():
        extractive_mod_summary = td_extract_summary(current_text)
        if not isinstance(extractive_mod_summary, str):
           print(f"Error: extractive_mod_summary is {type(extractive_mod_summary)} instead of str.")
           print(f"Value: {extractive_mod_summary}")
        inputs = tokenizer(extractive_mod_summary, max_length=1024, truncation=True, padding="max_length", return_tensors="pt")
        summary_ids = koios_model.generate(**inputs, max_length=200, min_length=50, length_penalty=2.0)
        abstractive_mod_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        final_summary[current_heading] = abstractive_mod_summary
print("\n **Final Summarized Sections**")
for heading, summary in final_summary.items():
  print(f"\n **{heading}**\n{summary}")

"""# Feedback from 1st Koios Model
This model does the job of summarization, highlighting key aspects. However, from reading the original text, some crucial information is still missing. This next set of training works on the extractive side, focusing on increasing selection of sentences so it can pick more while still using the same abstractive model to compare.

# Trying the LED pretrained model
"""

import torch
from transformers import LEDTokenizer, LEDForConditionalGeneration
from datasets import load_dataset

privacy_policies = "/content/drive/MyDrive/corpus/training/privacy_final_policies.jsonl"

dataset = load_dataset("json", data_files=privacy_policies, split="train")
print(dataset)

model_name="allenai/led-base-16384"
tokenizer=LEDTokenizer.from_pretrained("allenai/led-base-16384")
model=LEDForConditionalGeneration.from_pretrained(model_name)

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#Apply LoRA
from peft import get_peft_model, LoraConfig, TaskType
lora_config= LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=12,
    target_modules=["q_proj", "v_proj"],
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none"
)

model = get_peft_model(model, lora_config)

"""Preprocessing the Document"""

def preprocess_text(text):
  inputs=tokenizer(text["input"],max_length=1024, truncation=True,padding="max_length")
  labels=tokenizer(text["output"],max_length=512, truncation=True,padding="max_length")
  inputs["labels"]=labels["input_ids"]
  return inputs

dataset=dataset.map(preprocess_text,batched=True)
dataset.set_format(type="torch",columns=["input_ids","attention_mask", "labels"])

from transformers import Trainer, TrainingArguments
train_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/corpus/LEDTransformer",
    evaluation_strategy="no",
    learning_rate=1e-5,
    per_device_train_batch_size=1,
    num_train_epochs=15,
    save_strategy="epoch",
    fp16=False,
    logging_dir="./logs"
)

model_trainer = Trainer(
    model=model,
    args=train_args,
    train_dataset=dataset
)

model_trainer.train()

model.save_pretrained("/content/drive/MyDrive/corpus/LEDTransformer")
tokenizer.save_pretrained("/content/drive/MyDrive/corpus/LEDTransformer")

#practice document
policy= """
Thank you for using our services. Your privacy is important to us. This Privacy Agreement outlines how we collect, use, and protect your information.

We use cookies to enhance your browsing experience. Cookies are small text files stored on your device that help us improve website functionality and personalize content. By using our website, you consent to the use of cookies, which may include essential cookies necessary for website operation, performance cookies that help us understand user behavior and improve site performance, functional cookies that enhance user experience by remembering preferences, and advertising cookies that provide targeted advertisements based on browsing history. You can manage your cookie preferences through your browser settings. Disabling cookies may affect website functionality.

We collect and process personal data to provide and improve our services. The types of data we collect include personal information such as name, email address, and contact details when provided voluntarily; usage data, which includes information about how you interact with our website, such as IP address, browser type, and pages visited; and transaction data if you make a purchase, including payment details necessary to process transactions. We use this data to provide, maintain, and improve our services, personalize user experience, ensure website security and prevent fraud, and comply with legal obligations. We do not sell or share your data with third parties without your consent, except when required by law.
"""

policy2="""
A privacy agreement is a legal document that outlines how an organization collects, uses, stores, and protects user data. It serves as a contract between the company and its users, ensuring transparency in data handling practices. Privacy agreements typically cover details such as what types of personal information are collected, including names, email addresses, browsing history, and payment details. They also specify the purposes of data collection, whether for improving services, personalizing user experiences, or complying with legal obligations. Many privacy agreements include clauses about third-party data sharing, informing users whether their data will be shared with advertisers, analytics providers, or law enforcement. Additionally, they outline user rights, such as the ability to access, modify, or delete personal data, and often include instructions on how to exercise these rights. Security measures are another crucial aspect, detailing encryption methods, data retention policies, and protections against unauthorized access. With the rise of data breaches and cyber threats, privacy agreements help companies establish trust and demonstrate compliance with regulations like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). Companies must ensure that their privacy agreements are clear, easily accessible, and regularly updated to reflect changes in technology and legal requirements. Transparency is key, as vague or misleading agreements can result in legal consequences and damage to a companyâ€™s reputation. Users are encouraged to read privacy agreements carefully before consenting, as they provide insight into how their data is handled and whether they have control over it. Some companies offer opt-in or opt-out choices for data collection, giving users more control over their personal information. The complexity of privacy agreements can sometimes make them difficult to understand, which is why some organizations now provide simplified summaries alongside the full legal text. Businesses operating globally must account for differing privacy laws across jurisdictions, making compliance a continuous process. Privacy agreements also play a role in cookies and tracking technologies, informing users about their rights to manage cookie preferences. In some cases, companies may be required to obtain explicit consent before collecting certain types of data. As digital interactions continue to grow, privacy agreements will remain a fundamental aspect of online transactions and user trust. Organizations that prioritize privacy and data protection not only comply with legal standards but also foster stronger relationships with their users. Companies should also provide contact information for privacy-related inquiries, ensuring users have a way to address concerns or request clarifications. The evolving landscape of artificial intelligence and data analytics further underscores the importance of privacy agreements in safeguarding personal information. Ultimately, a well-crafted privacy agreement balances business needs with user rights, promoting ethical data practices in an increasingly digital world.
"""

"""# Testing LED Fine-Tuned Model"""

import torch
from transformers import LEDTokenizer, LEDForConditionalGeneration

LED_model_path="/content/drive/MyDrive/corpus/LEDTransformer"
tokenizer=LEDTokenizer.from_pretrained(LED_model_path)
LED_model=LEDForConditionalGeneration.from_pretrained(LED_model_path)

#extracting important sentences first so that it can extract that one instead of just choosing everything and truncating in the middle of a sentence
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

def extract_top_sentences(text, num_sentences=10):
    sentences = text.split(". ")  # Simple split on full stops(to indicate the end of a sentence)
    vectorizer = TfidfVectorizer()

    X = vectorizer.fit_transform(sentences)  # Converting to TF-IDF matrix
    td_scores = np.mean(X.toarray(), axis=1)  # Calculating the TF-IDF score per sentence

    top_sent_indices = np.argsort(td_scores)[-num_sentences:]  # Getting important sentences
    import_sentences = [sentences[i] for i in sorted(top_sent_indices)]

    return " ".join(import_sentences)

#Splitting the text into chunks so to feed it to the model so it can properly analyse the sentences
def split_text(text, split_size=4096, overlap=512):
    words = text.split()  # Split into words to avoid cutting mid-sentence
    chunks = []

    for i in range(0, len(words), split_size - overlap):
        chunk = " ".join(words[i : i + split_size])  # Reassemble chunk
        chunks.append(chunk)

    return chunks

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")

def sum_text(text):
    chunks = split_text(text)
    extracted_text = extract_top_sentences(" ".join(chunks))

    inputs = tokenizer(extracted_text, return_tensors="pt", padding=True, truncation=True, max_length=4096)
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    LED_model.to(device)

    summary_ids = LED_model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=600,
        min_length=200,
        length_penalty=1.0,
        num_beams=5,
        no_repeat_ngram_size=3
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

policy_sum=sum_text(policy2)
print("\nExtracted Summary:")
print(policy_sum)

"""# Abstractive Model- Flan-T5 (Fine-Tuned)"""

#importing
from datasets import load_dataset
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType

#Loading model
model_path="google/flan-t5-small"
tokenizer=T5Tokenizer.from_pretrained(model_path)
model=T5ForConditionalGeneration.from_pretrained(model_path)

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

privacy_policies = "/content/drive/MyDrive/corpus/training/privacy_final_policies.jsonl"

dataset = load_dataset("json", data_files=privacy_policies, split="train")
print(dataset)

#Apply LoRA
from peft import get_peft_model, LoraConfig, TaskType
lora_config= LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=12,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none"
)

model=get_peft_model(model, lora_config)

#tokenizer function, specifying that emojis should be added and that it should be in a fun way so that it matches the preferred output
def tokenizer_new (policies):
  inputs=["summarize with emojis: "+ text for text in policies["input"]]
  summaries=policies["output"]
  model_inputs=tokenizer(inputs,max_length=1024, truncation=True,padding="max_length")
  labels=tokenizer(summaries,max_length=512, truncation=True,padding="max_length")
  model_inputs["labels"]=labels["input_ids"]
  return model_inputs

dataset=dataset.map(tokenizer_new,batched=True)

"""Fine-tuning Flan T5"""

train_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/corpus/Flan-T5",
    evaluation_strategy="no",
    learning_rate=1e-5,
    gradient_accumulation_steps=8,
    weight_decay=0.01,
    per_device_train_batch_size=2,
    num_train_epochs=9,
    save_strategy="epoch",
    fp16=True,
    logging_dir="./logs"
)

model_trainer = Trainer(
    model=model,
    args=train_args,
    train_dataset=dataset
)

model_trainer.train()

model.save_pretrained("/content/drive/MyDrive/corpus/Flan-T5")
tokenizer.save_pretrained("/content/drive/MyDrive/corpus/Flan-T5")

"""# Saving Fine-Tuned Model"""

flan_model=T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/corpus/Flan-T5")
flan_tokenizer=T5Tokenizer.from_pretrained("/content/drive/MyDrive/corpus/Flan-T5")

"""# Testing The Fine-Tuned Model on Practice Paragraphs"""

def summarize_final_text(text):

  input_policy=prompt = "Summarize the key points of this privacy policy in simple terms: "+ text
  inputs=flan_tokenizer(input_policy, return_tensors="pt", max_length=512, truncation=True)
  summary_ids=flan_model.generate(inputs["input_ids"], max_length=128,min_length=50,length_penalty=1.0,num_beams=4,do_sample=True,
    top_k=40,top_p=0.7,repetition_penalty=1.8)
  return flan_tokenizer.decode(summary_ids[0],skip_special_tokens=True)

print(summarize_final_text(policy))

def summarize_final_text1(text):

  #input_policy=prompt = "Summarize the key points of this privacy policy in simple terms: "+ text
  inputs=flan_tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
  summary_ids=flan_model.generate(inputs["input_ids"], max_length=128,min_length=50,length_penalty=1.0,num_beams=4,do_sample=True,
  return flan_tokenizer.decode(summary_ids[0],skip_special_tokens=True)

print(summarize_final_text1(policy))

"""# Visual Feedback From Fine-Tuned Flan-T5 Small (Without ROUGE and BLEU testing)
The performance on the first practice paragraph was subpar. It summarised but it didn't use any articles or form any full sentences. It also does not include information that consumers may want to know

The performance of the model with different parameters in summarize_final_text1 is also subpar. The model only took one sentence out of the entire thing and repated it

# Trying Flan T5 Large without Fine-Tuning
"""

import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments

#Loading model
model_path="google/flan-t5-large"
tokenizer=T5Tokenizer.from_pretrained(model_path)
model=T5ForConditionalGeneration.from_pretrained(model_path)

def summarize_final_text(text):
  inputs=tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
  summary_ids=model.generate(inputs["input_ids"], max_length=200,min_length=50,length_penalty=1.0,num_beams=4,do_sample=True,  # Allow randomness
    top_k=40,top_p=0.7,repetition_penalty=1.8)
  return tokenizer.decode(summary_ids[0],skip_special_tokens=True)

#Save model
save_directory = "/content/drive/MyDrive/corpus/Flan-T5-large"

# Save model and tokenizer
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

generation_params = {
    "max_length": 200,
    "min_length": 50,
    "length_penalty": 1.0,
    "num_beams": 4,
    "do_sample": True
}
torch.save(generation_params, f"{save_directory}/generation_params.pth")

print("Model, tokenizer, and generation parameters saved!")

#practice document
policy= """
Thank you for using our services. Your privacy is important to us. This Privacy Agreement outlines how we collect, use, and protect your information.

We use cookies to enhance your browsing experience. Cookies are small text files stored on your device that help us improve website functionality and personalize content. By using our website, you consent to the use of cookies, which may include essential cookies necessary for website operation, performance cookies that help us understand user behavior and improve site performance, functional cookies that enhance user experience by remembering preferences, and advertising cookies that provide targeted advertisements based on browsing history. You can manage your cookie preferences through your browser settings. Disabling cookies may affect website functionality.

We collect and process personal data to provide and improve our services. The types of data we collect include personal information such as name, email address, and contact details when provided voluntarily; usage data, which includes information about how you interact with our website, such as IP address, browser type, and pages visited; and transaction data if you make a purchase, including payment details necessary to process transactions. We use this data to provide, maintain, and improve our services, personalize user experience, ensure website security and prevent fraud, and comply with legal obligations. We do not sell or share your data with third parties without your consent, except when required by law.
"""

print(summarize_final_text(policy))

new_policy="""
At Pinterest, we collect information to improve our services, enhance user experience, and ensure security. When you visit our website or use our services, we automatically gather data such as your IP address, browser type, device information, and browsing behavior through cookies and similar tracking technologies. If you create an account or make a purchase, we collect personal details like your name, email address, payment information, and shipping details. Additionally, when you interact with our customer support or provide feedback, we store your communications to assist you better. We may also collect data from third-party sources, such as social media platforms or marketing partners, to personalize your experience and provide relevant offers. Usage data, including how you navigate our website and which features you use, helps us refine our products and tailor content to your preferences. If you subscribe to our newsletter or promotional emails, we track engagement metrics like open rates and link clicks to optimize our communication. We ensure that all data collection follows legal guidelines and industry standards, and we give users control over their privacy settings. You can manage cookie preferences, update personal information, or opt out of marketing communications at any time. For security purposes, we monitor login activity and may use automated tools to detect fraud or suspicious behavior. Our data collection practices help us provide seamless service, troubleshoot technical issues, and comply with regulatory requirements. We also anonymize and aggregate certain data for analytical purposes, ensuring that individual identities remain protected. Your trust is important to us, and we are committed to transparency in how we collect, store, and use your information. By using our services, you consent to our data collection practices, which are detailed in our Privacy Policy. If you have any questions or concerns about how we handle your data, you can contact our support team or review our privacy settings.
"""

print(summarize_final_text(new_policy))

"""# Visual Assessment of Summaries From Flan T5- Large Without Fine Tuning
This model with a certain number of parameters seems to work better. Even though it does more of an extraction than rewording text

# Second Koios Final Pipeline (LED+ Flan T5 Large)
"""

#Practice document
url="/content/drive/MyDrive/corpus/htmltest/www8_hp.html"

#importing libraries
from transformers import LEDTokenizer, LEDForConditionalGeneration
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

#Loading LED model
LED_path="/content/drive/MyDrive/corpus/LEDTransformer"
LED_model=LEDForConditionalGeneration.from_pretrained(LED_path)
LED_tokenizer=LEDTokenizer.from_pretrained(LED_path)

t5_path="/content/drive/MyDrive/corpus/Flan-T5-large"
t5_model=T5ForConditionalGeneration.from_pretrained(t5_path)
t5_tokenizer=T5Tokenizer.from_pretrained(t5_path)

def extraction_function_text(html):
    from bs4 import BeautifulSoup

    with open(html, "r", encoding="utf-8") as file:
        content = file.read()
        soup = BeautifulSoup(content, "html.parser")

    sections = {}
    current_heading = None
    current_text = []

    for elem in soup.find_all(["h1", "h2", "h3", "p"]):

        if elem.name in ["h1", "h2", "h3"]:
            if current_heading:
                sections[current_heading] = " ".join(current_text).strip()
            current_heading = elem.get_text().strip()
            current_text = []
        else:
            current_text.append(elem.get_text().strip())

    if current_heading and current_text:
        sections[current_heading] = " ".join(current_text).strip()


    return sections

#abstractive function
def abstract_final_text(text):


  inputs=t5_tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
  summary_ids=t5_model.generate(inputs["input_ids"], max_length=200,min_length=50,length_penalty=1.0,num_beams=4,do_sample=True,
    top_k=40,top_p=0.7,repetition_penalty=1.8)
  return t5_tokenizer.decode(summary_ids[0],skip_special_tokens=True)

#extractive function
def extract_final_text(text):
    chunks = split_text(text)
    extracted_text = extract_top_sentences(" ".join(chunks))

    inputs = LED_tokenizer(extracted_text, return_tensors="pt", padding=True, truncation=True, max_length=4096)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]


    summary_ids = LED_model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=600,
        min_length=200,
        length_penalty=1.0,
        num_beams=5,
        no_repeat_ngram_size=3
    )

    return LED_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

import torch

def koios_pipeline2(url, LED_model, LED_tokenizer, t5_model, t5_tokenizer):

    # Extract text from HTML
    extracted_summary = extraction_function(url)

    final_summary = {}

    # Loop through sections and summarize
    for current_heading, current_text in extracted_summary.items():

        # Ensure text is valid
        if not isinstance(current_text, str) or not current_text.strip():
            print(f"Skipping empty or invalid section: {current_heading}")
            continue

        try:
            #  Extractive Summarization using LED model
            inputs = LED_tokenizer(current_text, max_length=4096, truncation=True, return_tensors="pt")
            summary_ids = LED_model.generate(**inputs, max_length=200, min_length=50, length_penalty=2.0)
            extractive_summary =LED_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

            # Ensure extractive summary is valid
            if not isinstance(extractive_summary, str) or not extractive_summary.strip():
                print(f"Error: extractive_summary is {type(extractive_summary)} instead of str for section {current_heading}")
                continue

            # Abstractive Summarization using T5 model
            inputs = t5_tokenizer(extractive_summary, max_length=1024, truncation=True, padding="max_length", return_tensors="pt")
            summary_ids = t5_model.generate(**inputs, max_length=200, min_length=50, length_penalty=2.0)
            abstractive_summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)

            # Store final summary
            final_summary[current_heading] = abstractive_summary

        except Exception as e:
            print(f"Error processing section '{current_heading}': {e}")

    return final_summary

#new one using pre defined functions

def koios_pipeline2(url, LED_model, LED_tokenizer, t5_model, t5_tokenizer):


    extracted_summary = extraction_function(url)

    final_summary = {}


    for current_heading, current_text in extracted_summary.items():


        if not isinstance(current_text, str) or not current_text.strip():
            print(f"Skipping empty or invalid section: {current_heading}")
            continue

        try:
            # Extractive Summarization using LED model
            extractive_summary =extract_final_text(current_text)

            # Ensure extractive summary is valid
            if not isinstance(extractive_summary, str) or not extractive_summary.strip():
                print(f"Error: extractive_summary is {type(extractive_summary)} instead of str for section {current_heading}")
                continue

            #  Abstractive Summarization using T5 model
            abstractive_summary =abstract_final_text(extractive_summary)

            # Store final summary
            final_summary[current_heading] = abstractive_summary

        except Exception as e:
            print(f"Error processing section '{current_heading}': {e}")

    return final_summary

final_summary = koios_pipeline2(url, LED_model, LED_tokenizer, t5_model, t5_tokenizer)

# Print the results
print("\n **Final Summarized Sections**")
for heading, summary in final_summary.items():
    print(f"\n **{heading}**\n{summary}")