# -*- coding: utf-8 -*-
"""Final  Koios Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e3lfqwvq-chD6dJKH2a6shcQHv7RQHl0

# Koios Model
This model is a mix of extractive and abstractive summarization. Extractive summarization is meant to extract the most important phrases from the main text. The important sentences are then passed through the abstractive model (built using BART) to generate a user-friendly summary

# Installing libraries
"""

pip install torch transformers datasets accelerate peft bitsandbytes

pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html

pip install bs4

pip install sentence_transformers

pip install nltk

"""# Extraction Function
This function is meant to scrape the HTML from the site and feed it into the model
"""

def extraction_function(html):
    from bs4 import BeautifulSoup

    with open(html, "r", encoding="utf-8") as file:
        content = file.read()
        soup = BeautifulSoup(content, "html.parser")

    sections = {}
    current_heading = None
    current_text = []  # Initialize

    for elem in soup.find_all(["h1", "h2", "h3", "p"]):

        if elem.name in ["h1", "h2", "h3"]:
            if current_heading:
                sections[current_heading] = " ".join(current_text).strip()
            current_heading = elem.get_text().strip()
            current_text = []
        else:
            current_text.append(elem.get_text().strip())

    if current_heading and current_text:
        sections[current_heading] = " ".join(current_text).strip()


    return sections

"""Testing the extraction function"""

html_file= "/content/drive/MyDrive/corpus/htmltest/www8_hp.html"
policy=extraction_function(html_file)
for section, text in policy.items():
    print(f"## {section} ###\n{text}\n")

"""# Extractive Summarization
This uses TD-IF to select the sentences with the highest importance. The measurement for importance is based on the privacy terms
"""

import torch
import nltk
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

nltk.download('stopwords')

nltk.download('punkt_tab')

sent_model=SentenceTransformer("all-MiniLM-L6-v2")

#Key privacy terms to prioritize
privacy_keywords={
    "privacy","data","personal","information","collection","third-party","cookies","retention",
    "cookies","sharing","consent","security","policy","tracking", "advertising","partners",
    "compliance","regulation","opt-out", "security"
}

#Legal phrases that should be kept
legal_phrases={
    "third-party sharing", "right to be forgotten","legitimate interest","data retention",
    "opt-out option","cookie tracking"
}

#Stopwords to remove to improve TD-IDF scores but keep the ones that are needed for legal phrases (i.e to be)
stopwords=list(set(nltk.corpus.stopwords.words("english")).union({"shall","hereby","thereof","whereas"}) - set("to be".split()))

#Function to preprocess text, keeping legal phrases. Changed text to a single token
def preprocess_text(text):
    for phrase in legal_phrases:
        text=text.replace(phrase,phrase.replace(" ","_"))
    return text

def td_extract_summary(text, boost_factor=2.0):
    text = preprocess_text(text)
    sentences = sent_tokenize(text)
    total_sentences = len(sentences)

    if total_sentences < 5:
        return text
    if total_sentences <= 10:
        num_sentences = max(5, int(total_sentences * 0.80))
    elif total_sentences <= 20:
        num_sentences = max(9, int(total_sentences * 0.5))
    else:
        num_sentences = max(11, int(total_sentences * 0.4))

    # TF-IDF
    vectorizer = TfidfVectorizer(stop_words=stopwords)
    tdidf_matrix = vectorizer.fit_transform(sentences)
    tdidf_scores = np.asarray(tdidf_matrix.mean(axis=0)).flatten()
    word_to_tdidf = dict(zip(vectorizer.get_feature_names_out(), tdidf_scores))

    sentence_weights = []
    for sentence in sentences:
        words = sentence.lower().split()
        weight = sum(word_to_tdidf.get(word, 0) for word in words)
        if any(word in privacy_keywords for word in words):
            weight *= boost_factor
        sentence_weights.append(weight)

    sentence_weights = np.array(sentence_weights)
    max_weight = np.max(sentence_weights) if np.max(sentence_weights) > 0 else 1
    sentence_weights = (sentence_weights / max_weight) * boost_factor

    # Encode sentences
    sentence_embeddings = sent_model.encode(sentences, convert_to_tensor=True)
    doc_embedding = torch.mean(sentence_embeddings, dim=0)

    # Compute similarities
    similarities = torch.nn.functional.cosine_similarity(sentence_embeddings, doc_embedding.unsqueeze(0))

    # Move sentence_weights to the same device as similarities
    sentence_weights_tensor = torch.tensor(sentence_weights, dtype=torch.float32, device=similarities.device)
    similarities = similarities * sentence_weights_tensor  # No more device mismatch!

    # Select top-ranked sentences
    top_indices = torch.topk(similarities, num_sentences).indices.cpu().numpy()  # Move to CPU for indexing

    # Maintain original order of selected sentences
    top_sentences = [sentences[i].replace("_", " ") for i in sorted(top_indices)]  # Restore legal phrases

    return " ".join(top_sentences)

"""# Abstractive Summarization

# PEGASUS
"""

from transformers import PegasusForConditionalGeneration, PegasusTokenizer
model_name = "google/pegasus-large"
ptokenizer = PegasusTokenizer.from_pretrained(model_name)
pmodel = PegasusForConditionalGeneration.from_pretrained(model_name)

"""# Final Pipeline (TD-IDF+Pegasus)

"""

def sum_text(input_text):
    if not input_text or len(input_text.strip()) == 0:
        print("Warning: Empty input text for summarization.")
        return "No summary available."

    inputs = ptokenizer(input_text, truncation=True, max_length=1024, return_tensors="pt")

    summary_ids = pmodel.generate(
        **inputs,
        max_length=150,
        min_length=50,
        num_beams=3,  # Reduce beams to allow more variation
        length_penalty=1.5,  # Encourage better-structured sentences
        repetition_penalty=2.0,  # Prevent repeating words
        no_repeat_ngram_size=3,  # Avoid repeating phrases
        temperature=0.9,  # Increases diversity (higher = more random)
        top_k=50,  # Sample from the top 50 words instead of picking the best
        top_p=0.85,  # Allows some randomness while keeping coherence
        do_sample=True  # Enables sampling (more natural summaries)
    )

    summary = ptokenizer.decode(summary_ids[0], skip_special_tokens=True)

    if not summary or len(summary.strip()) == 0:
        print("Warning: Model returned an empty summary.")
        return "No summary available."

    return summary

#works
url = "/content/drive/MyDrive/corpus/htmltrain/www_apple.html"
extracted_summary = extraction_function(url)

if not extracted_summary:
    print("Error: No text extracted from the document.")
    exit()

final_summary = {}

for heading, text in extracted_summary.items():
    extractive_mod_summary = td_extract_summary(text)

    if not isinstance(extractive_mod_summary, str):
        print(f"Error: Extractive summary is {type(extractive_mod_summary)} instead of str.")
        continue

    # Ensure input is properly formatted
    cleaned_text = extractive_mod_summary.replace("\n", " ").strip()

    # Use the working abstractive function
    abstractive_mod_summary = sum_text(cleaned_text)

    final_summary[heading] = abstractive_mod_summary

# Print the final summaries
for heading, summary in final_summary.items():
    print(f"\n **{heading}**\n{summary}")

"""#Refining With GPT"""

import openai

def refined_text(text):
  client=openai.OpenAI(api_key=openai.api_key)

  response=client.chat.completions.create(
      model="gpt-4-turbo",
      messages=[{"role": "user", "content":f"Rewrite the folowing privact policy section using emojis an simple and engaging language:\n\n{text}"}],
      max_tokens=150
  )
  return response.choices[0].message.content

#with refinement from GPT
#works
url = "/content/drive/MyDrive/corpus/htmltrain/www_apple.html"
extracted_summary = extraction_function(url)

if not extracted_summary:
    print("Error: No text extracted from the document.")
    exit()

final_summary = {}

for heading, text in extracted_summary.items():
    extractive_mod_summary = td_extract_summary(text)

    if not isinstance(extractive_mod_summary, str):
        print(f"Error: Extractive summary is {type(extractive_mod_summary)} instead of str.")
        continue

    # Ensure input is properly formatted
    cleaned_text = extractive_mod_summary.replace("\n", " ").strip()

    # Use the working abstractive function
    abstractive_mod_summary = sum_text(cleaned_text)

    refined_summary=refined_text(abstractive_mod_summary)

    final_summary[heading] = refined_summary

# Print the final summaries
for heading, summary in final_summary.items():
    print(f"\n **{heading}**\n{summary}")

"""# Flesch-Kincaid Test For Summary Readability"""

#installing the textstat library for Flesch and Flesch-Kincaid Scores
!pip install textstat

import textstat

#testing section by section to identify if there are any difficult sections
summary_sections=[
    "Who's in charge of keeping your info safe when you use our website? Most of the time, it's ABL! But, if you're buying tickets, signing up for memberships, getting goodies from our store, or involved in any other cool stuff with us like our soccer schools, then it's Arsenal who keeps your data secure.",
    "Hey kiddos under 13! Wanna join in on fun contests and snag cool prizes? Make sure your parent or guardian signs you up! If you win, we’ll tell them first (to Mom or Dad!) to keep things super safe. Under 18 and won something? Awesome! We’ll need a thumbs-up from your parents before we share any of your personal stuff to celebrate your win!Remember, always play it safe and fun!",
    "Hey there! Just a heads up, we occasionally update our privacy and security policies.Make sure to swing by this page once in a while to stay in the loop, or you can hit up our Data Protection Officer to snag the latest copy! " ,
    "Hey there! Need to fix or update your info? It's easy! Just tweak your profile settings online or shoot a message to our Data Protection Officer (details below).Keep your details fresh and up-to-date to stay in the loop! And don’t forget, you can also find us hanging out on Facebook, Twitter, and Google+!",
    "Hey there! Just so you know: We might ask for a small fee (£10.00) to help cover our costs when you request updates or want to see your info.You can ask us anytime to fix your Personal Information to keep it accurate and fresh! You have the right to check out the details of the info we have about you. Keep it simple and stay informed!",
    "Sometimes, to get you the stuff you need, we send your info to other places where privacy laws might not be as strong as in the UK. But don't worry! We'll do our best to keep your information safe and sound, no matter where it goes!",
    "We might share your info with our trusted buddies - like sponsors, money helpers, and others we need to help get your orders to you, keep our systems running smooth, or follow the law.",
    "This includes your name, where you live, your phone number, when you were born, credit card details, where we should send bills, your email, and what you like to buy. We promise to use this info carefully and keep it safe, just like we said in our Privacy Policy.",
    "When you use our website or chat with us through other ways, if you share your info, it means you're cool with us using it based on our privacy rules.If you're not okay with it, just let us know!",
    "Your info is super safe on our secure server! Only a few of our team members have the key, and they can only peek when they really need to. Plus, we've got strong rules and steps to keep your data safe from any oopsies or nosy people! We only use your info carefully and wisely."
]

for idx, paragraph in enumerate(summary_sections):
  flesch_score=textstat.flesch_reading_ease(paragraph)
  flesch_k_grade=textstat.flesch_kincaid_grade(paragraph)
  print(f"Section {idx+1}:Flesch={flesch_score:.2f}, Flesch-Kincaid Grade={flesch_k_grade:.2f}")

